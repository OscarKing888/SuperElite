# -*- coding: utf-8 -*-
"""
Co-Instruct å•åŠŸèƒ½æµ‹è¯• - ä¸­æ–‡æ ‡é¢˜
"""

import torch
from pathlib import Path
from PIL import Image

# æµ‹è¯•å›¾ç‰‡
TEST_IMAGE = "/Volumes/990PRO4TB/2025/2025-09-20/_Z8L1493.NEF"

print("ğŸ“· Co-Instruct æµ‹è¯• - ä¸­æ–‡æ ‡é¢˜ç”Ÿæˆ")
print("=" * 50)
print(f"æµ‹è¯•å›¾ç‰‡: {TEST_IMAGE}")

# åŠ è½½å›¾ç‰‡
print("\næ­£åœ¨åŠ è½½å›¾ç‰‡...")
import rawpy
import io
with rawpy.imread(TEST_IMAGE) as raw:
    thumb = raw.extract_thumb()
    image = Image.open(io.BytesIO(thumb.data)).convert("RGB")

# ç¼©å°
w, h = image.size
if max(w, h) > 672:
    if w > h:
        new_w, new_h = 672, int(h * 672 / w)
    else:
        new_h, new_w = 672, int(w * 672 / h)
    image = image.resize((new_w, new_h), Image.LANCZOS)
print(f"å›¾ç‰‡å°ºå¯¸: {image.size}")

# åŠ è½½æ¨¡å‹
print("\næ­£åœ¨åŠ è½½æ¨¡å‹...")
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "q-future/co-instruct",
    trust_remote_code=True,
    torch_dtype=torch.float16,
    attn_implementation="eager",
    device_map={"": "mps"}
)
print("æ¨¡å‹åŠ è½½å®Œæˆ")

# æµ‹è¯•ä¸­æ–‡æ ‡é¢˜
print("\n" + "=" * 50)
print("ğŸ”¹ æµ‹è¯•: ä¸­æ–‡æ ‡é¢˜ç”Ÿæˆ")
print("=" * 50)

import time
prompt = "USER: The image: <|image|> ä¸ºè¿™å¼ ç…§ç‰‡åˆ›ä½œä¸€ä¸ªå¯Œæœ‰è¯—æ„çš„ä¸­æ–‡æ ‡é¢˜ï¼Œ5-10ä¸ªå­—ã€‚ ASSISTANT:"

start = time.time()
response = model.chat(prompt, [image], max_new_tokens=50)
elapsed = time.time() - start

print(f"\nğŸ“Œ å“åº”ç±»å‹: {type(response)}")
print(f"ğŸ“Œ å“åº”å†…å®¹: {response}")
print(f"â±ï¸ è€—æ—¶: {elapsed:.1f}s")
