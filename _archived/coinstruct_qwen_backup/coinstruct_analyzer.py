# -*- coding: utf-8 -*-
"""
Co-Instruct åˆ†æå™¨
ç”¨äºç”Ÿæˆå…³é”®å­—ã€åœºæ™¯æè¿°ã€æ ‡é¢˜ç­‰å…ƒæ•°æ®
"""

import os
import time
import torch
from pathlib import Path
from PIL import Image
from typing import Optional, Dict, Any

# æ¨¡å‹å•ä¾‹
_model = None
_model_loading = False


def get_model():
    """è·å–æˆ–åŠ è½½ Co-Instruct æ¨¡å‹ï¼ˆå•ä¾‹ï¼‰"""
    global _model, _model_loading
    
    if _model is not None:
        return _model
    
    if _model_loading:
        # é¿å…é‡å¤åŠ è½½
        while _model_loading and _model is None:
            time.sleep(0.5)
        return _model
    
    _model_loading = True
    
    try:
        print("[Co-Instruct] æ­£åœ¨åŠ è½½æ¨¡å‹...")
        from transformers import AutoModelForCausalLM
        
        _model = AutoModelForCausalLM.from_pretrained(
            "q-future/co-instruct",
            trust_remote_code=True,
            torch_dtype=torch.float16,
            attn_implementation="eager",
            device_map={"": "mps"}
        )
        print("[Co-Instruct] æ¨¡å‹åŠ è½½å®Œæˆ")
        return _model
    except Exception as e:
        print(f"[Co-Instruct] æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise
    finally:
        _model_loading = False


def unload_model():
    """å¸è½½æ¨¡å‹é‡Šæ”¾å†…å­˜"""
    global _model
    if _model is not None:
        del _model
        _model = None
        torch.mps.empty_cache()
        print("[Co-Instruct] æ¨¡å‹å·²å¸è½½")


def prepare_image(image_path: str) -> Image.Image:
    """
    å‡†å¤‡å›¾ç‰‡ç”¨äºåˆ†æ
    æ”¯æŒ RAW å’Œå¸¸è§„å›¾ç‰‡æ ¼å¼ï¼ŒRAW ä½¿ç”¨å†…åµŒé¢„è§ˆå›¾
    """
    path = Path(image_path)
    
    if not path.exists():
        raise FileNotFoundError(f"å›¾ç‰‡ä¸å­˜åœ¨: {image_path}")
    
    # RAW æ ¼å¼éœ€è¦è½¬æ¢
    raw_extensions = {'.cr2', '.cr3', '.nef', '.arw', '.orf', '.raf', '.rw2', '.dng', '.pef', '.raw'}
    
    if path.suffix.lower() in raw_extensions:
        # æ£€æŸ¥æ˜¯å¦æœ‰åŒå JPG
        for ext in ['.jpg', '.jpeg', '.JPG', '.JPEG']:
            jpg_path = path.with_suffix(ext)
            if jpg_path.exists():
                return Image.open(jpg_path).convert("RGB")
        
        # ä½¿ç”¨ rawpy æå–å†…åµŒç¼©ç•¥å›¾ï¼ˆæ›´å¿«ï¼‰
        try:
            import rawpy
            import io
            with rawpy.imread(str(path)) as raw:
                thumb = raw.extract_thumb()
                if thumb.format == rawpy.ThumbFormat.JPEG:
                    return Image.open(io.BytesIO(thumb.data)).convert("RGB")
                elif thumb.format == rawpy.ThumbFormat.BITMAP:
                    return Image.fromarray(thumb.data).convert("RGB")
                else:
                    # æ— æ³•æå–ç¼©ç•¥å›¾ï¼Œä½¿ç”¨å®Œæ•´è§£ç 
                    rgb = raw.postprocess()
                    return Image.fromarray(rgb).convert("RGB")
        except ImportError:
            raise RuntimeError("éœ€è¦å®‰è£… rawpy å¤„ç† RAW æ–‡ä»¶")
    else:
        return Image.open(image_path).convert("RGB")


def resize_for_analysis(image: Image.Image, max_size: int = 672) -> Image.Image:
    """è°ƒæ•´å›¾ç‰‡å¤§å°ä»¥åŠ å¿«åˆ†æé€Ÿåº¦"""
    w, h = image.size
    if max(w, h) <= max_size:
        return image
    
    if w > h:
        new_w = max_size
        new_h = int(h * max_size / w)
    else:
        new_h = max_size
        new_w = int(w * max_size / h)
    
    return image.resize((new_w, new_h), Image.LANCZOS)


# ==================== Prompts ====================

PROMPTS = {
    "keywords_en": "USER: The image: <|image|> Generate 10 descriptive keywords for this photograph, separated by commas. ASSISTANT:",
    
    "keywords_cn": "USER: The image: <|image|> ä¸ºè¿™å¼ ç…§ç‰‡ç”Ÿæˆ10ä¸ªæè¿°æ€§å…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”ã€‚ ASSISTANT:",
    
    "caption_en": "USER: The image: <|image|> Describe this photograph in 2-3 sentences. Focus on the subject, setting, lighting, and mood. ASSISTANT:",
    
    "caption_cn": "USER: The image: <|image|> ç”¨2-3å¥è¯æè¿°è¿™å¼ ç…§ç‰‡çš„åœºæ™¯å’Œæ°›å›´ã€‚ ASSISTANT:",
    
    "title_en": "USER: The image: <|image|> Create a poetic title for this photograph in 3-6 words. ASSISTANT:",
    
    "title_cn": "USER: The image: <|image|> ä¸ºè¿™å¼ ç…§ç‰‡åˆ›ä½œä¸€ä¸ªå¯Œæœ‰è¯—æ„çš„ä¸­æ–‡æ ‡é¢˜ï¼Œ5-10ä¸ªå­—ã€‚ ASSISTANT:",
    
    "scene": "USER: The image: <|image|> Classify this photograph into one category: sunset, sunrise, mountain, ocean, forest, city, wildlife, portrait, street, architecture, night, aurora, waterfall, desert, lake, field, sky, abstract. Answer with one word only. ASSISTANT:",
    
    "mood": "USER: The image: <|image|> Describe the mood and atmosphere of this photograph in 2-3 words, such as: peaceful, dramatic, mysterious, romantic, melancholic, energetic, serene. ASSISTANT:",
}


def analyze(
    image_path: str,
    tasks: list = None,
    language: str = "cn"  # "cn" æˆ– "en"
) -> Dict[str, Any]:
    """
    åˆ†æå›¾ç‰‡ï¼Œç”Ÿæˆå…ƒæ•°æ®
    
    Args:
        image_path: å›¾ç‰‡è·¯å¾„
        tasks: è¦æ‰§è¡Œçš„ä»»åŠ¡åˆ—è¡¨ ['keywords', 'caption', 'title', 'scene', 'mood']
               é»˜è®¤å…¨éƒ¨æ‰§è¡Œ
        language: è¯­è¨€åå¥½ "cn" æˆ– "en"
    
    Returns:
        {
            "success": True,
            "keywords": "...",
            "caption": "...",
            "title": "...",
            "scene": "...",
            "mood": "...",
            "processing_time": 12.5
        }
    """
    if tasks is None:
        tasks = ["keywords", "caption", "title", "scene", "mood"]
    
    start_time = time.time()
    result = {"success": False}
    
    try:
        # åŠ è½½æ¨¡å‹
        model = get_model()
        
        # å‡†å¤‡å›¾ç‰‡
        image = prepare_image(image_path)
        image = resize_for_analysis(image)
        
        # æ‰§è¡Œå„é¡¹åˆ†æ
        for task in tasks:
            prompt_key = task
            
            # é€‰æ‹©è¯­è¨€ç‰ˆæœ¬
            if task in ["keywords", "caption", "title"]:
                prompt_key = f"{task}_{language}"
            
            if prompt_key not in PROMPTS:
                continue
            
            prompt = PROMPTS[prompt_key]
            
            try:
                response = model.chat(prompt, [image], max_new_tokens=150)
                
                # æ¸…ç†å“åº”
                if isinstance(response, str):
                    response = response.strip()
                else:
                    response = str(response).strip()
                
                result[task] = response
                
            except Exception as e:
                result[task] = f"[Error: {e}]"
        
        result["success"] = True
        result["processing_time"] = round(time.time() - start_time, 2)
        
    except Exception as e:
        result["error"] = str(e)
        result["processing_time"] = round(time.time() - start_time, 2)
    
    return result


# ==================== æµ‹è¯• ====================

if __name__ == "__main__":
    import sys
    
    # æŸ¥æ‰¾æµ‹è¯•å›¾ç‰‡
    test_dir = Path(__file__).parent.parent / "test_photos"
    test_image = None
    
    if test_dir.exists():
        for f in test_dir.iterdir():
            if f.suffix.lower() in ['.jpg', '.jpeg', '.png']:
                test_image = str(f)
                break
    
    if test_image is None:
        print("è¯·æä¾›æµ‹è¯•å›¾ç‰‡è·¯å¾„ä½œä¸ºå‚æ•°")
        print("ç”¨æ³•: python coinstruct_analyzer.py /path/to/image.jpg")
        sys.exit(1)
    
    print(f"ğŸ“· æµ‹è¯•å›¾ç‰‡: {test_image}")
    print("=" * 60)
    
    result = analyze(test_image, language="cn")
    
    print("\n" + "=" * 60)
    print("ğŸ“Š åˆ†æç»“æœ:")
    print("=" * 60)
    
    for key, value in result.items():
        print(f"\nğŸ”¹ {key}:")
        print(f"   {value}")
