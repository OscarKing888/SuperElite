# -*- coding: utf-8 -*-
"""
Qwen3-VL-8B-Instruct MLX æµ‹è¯• (ä¿®æ­£ç‰ˆ)
æµ‹è¯•ç”¨äºæ›¿ä»£ Co-Instruct çš„å›¾åƒæè¿°å’Œæ ‡é¢˜ç”ŸæˆåŠŸèƒ½
"""

import time
import os
import json
from pathlib import Path

# æµ‹è¯•å›¾ç‰‡
TEST_IMAGE = "/Volumes/990PRO4TB/2025/2025-09-20/_Z8L1493.NEF"

print("ğŸ“· Qwen3-VL-8B-Instruct MLX æµ‹è¯•")
print("=" * 60)
print(f"æµ‹è¯•å›¾ç‰‡: {TEST_IMAGE}")

# ==================== è®¡æ—¶å¼€å§‹ ====================
total_start = time.time()

# æ­¥éª¤ 1: åŠ è½½å›¾ç‰‡
print("\nâ±ï¸ æ­¥éª¤ 1: åŠ è½½å›¾ç‰‡...")
step_start = time.time()

from PIL import Image
import rawpy
import io

with rawpy.imread(TEST_IMAGE) as raw:
    thumb = raw.extract_thumb()
    image = Image.open(io.BytesIO(thumb.data)).convert("RGB")

# ç¼©å°å›¾ç‰‡
w, h = image.size
if max(w, h) > 672:
    if w > h:
        new_w, new_h = 672, int(h * 672 / w)
    else:
        new_h, new_w = 672, int(w * 672 / h)
    image = image.resize((new_w, new_h), Image.LANCZOS)

# ä¿å­˜ä¸´æ—¶å›¾ç‰‡ä¾› mlx-vlm ä½¿ç”¨
temp_image_path = "/tmp/test_image_for_qwen.jpg"
image.save(temp_image_path, "JPEG", quality=95)

image_load_time = time.time() - step_start
print(f"   å›¾ç‰‡å°ºå¯¸: {image.size}")
print(f"   âœ… ç”¨æ—¶: {image_load_time:.2f}s")

# æ­¥éª¤ 2: åŠ è½½æ¨¡å‹
print("\nâ±ï¸ æ­¥éª¤ 2: åŠ è½½ Qwen3-VL-8B-Instruct MLX æ¨¡å‹...")
step_start = time.time()

from mlx_vlm import load, generate
from mlx_vlm.prompt_utils import apply_chat_template
from mlx_vlm.utils import load_config

MODEL_PATH = "lmstudio-community/Qwen3-VL-8B-Instruct-MLX-8bit"

# åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
model, processor = load(MODEL_PATH)
config = load_config(MODEL_PATH)

model_load_time = time.time() - step_start
print(f"   âœ… ç”¨æ—¶: {model_load_time:.2f}s")

# æ­¥éª¤ 3: ç”Ÿæˆä¸­æ–‡æ ‡é¢˜
print("\nâ±ï¸ æ­¥éª¤ 3: ç”Ÿæˆä¸­æ–‡æ ‡é¢˜...")
step_start = time.time()

prompt_title = "ä¸ºè¿™å¼ ç…§ç‰‡åˆ›ä½œä¸€ä¸ªå¯Œæœ‰è¯—æ„çš„ä¸­æ–‡æ ‡é¢˜ï¼Œ5-10ä¸ªå­—ã€‚åªè¾“å‡ºæ ‡é¢˜ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"
formatted_prompt = apply_chat_template(processor, config, prompt_title, num_images=1)

# æ­£ç¡®çš„ generate è°ƒç”¨æ–¹å¼ï¼šimage å‚æ•°è¦å•ç‹¬ä¼ 
title_response = generate(
    model, 
    processor, 
    formatted_prompt,
    image=[temp_image_path],  # ä½¿ç”¨ image å…³é”®å­—å‚æ•°
    max_tokens=50,
    verbose=False
)

title_time = time.time() - step_start
print(f"   âœ… ç”¨æ—¶: {title_time:.2f}s")

# æ­¥éª¤ 4: ç”Ÿæˆè¯¦ç»†æè¿°
print("\nâ±ï¸ æ­¥éª¤ 4: ç”Ÿæˆè¯¦ç»†ç”»é¢æè¿°...")
step_start = time.time()

prompt_desc = """è¯·è¯¦ç»†æè¿°è¿™å¼ ç…§ç‰‡çš„ç”»é¢å†…å®¹ï¼ŒåŒ…æ‹¬ï¼š
1. ä¸»ä½“æ˜¯ä»€ä¹ˆ
2. ç¯å¢ƒå’ŒèƒŒæ™¯
3. å…‰çº¿æ¡ä»¶
4. è‰²å½©ç‰¹ç‚¹
5. ç”»é¢æ°›å›´å’Œæƒ…æ„Ÿ

ç”¨ä¸­æ–‡å›ç­”ï¼Œå°½å¯èƒ½è¯¦ç»†ã€‚"""

formatted_prompt_desc = apply_chat_template(processor, config, prompt_desc, num_images=1)

desc_response = generate(
    model, 
    processor, 
    formatted_prompt_desc,
    image=[temp_image_path],
    max_tokens=300,
    verbose=False
)

desc_time = time.time() - step_start
print(f"   âœ… ç”¨æ—¶: {desc_time:.2f}s")

# ==================== ç»“æœæ±‡æ€» ====================
total_time = time.time() - total_start

print("\n" + "=" * 60)
print("ğŸ“Œ ç”Ÿæˆç»“æœ")
print("=" * 60)
print(f"\nğŸ·ï¸ ä¸­æ–‡æ ‡é¢˜:")
print(f"   {title_response}")

print(f"\nğŸ“ ç”»é¢æè¿°:")
print("-" * 40)
print(desc_response)

print("\n" + "=" * 60)
print("ğŸ“Š ç”¨æ—¶ç»Ÿè®¡")
print("=" * 60)
print(f"   å›¾ç‰‡åŠ è½½:   {image_load_time:>6.2f}s")
print(f"   æ¨¡å‹åŠ è½½:   {model_load_time:>6.2f}s")
print(f"   æ ‡é¢˜ç”Ÿæˆ:   {title_time:>6.2f}s")
print(f"   æè¿°ç”Ÿæˆ:   {desc_time:>6.2f}s")
print("-" * 30)
print(f"   æ€»ç”¨æ—¶:     {total_time:>6.2f}s")
print("=" * 60)

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
if os.path.exists(temp_image_path):
    os.remove(temp_image_path)
